Memory Augmented Large Language Models are
Computationally Universal
Dale Schuurmans
Google Brain & University of Alberta
Abstract
We show that transformer-based large language models are co mputationally univer-
sal when augmented with an external memory. Any determinist ic language model that
conditions on strings of bounded length is equivalent to a ﬁn ite automaton, hence com-
putationally limited. However, augmenting such models wit h a read-write memory cre-
atesthepossibilityofprocessingarbitrarilylargeinput sand,potentially, simulatingany
algorithm. We establish that an existing large language mod el, Flan-U-PaLM 540B,
can be combined with an associative read-write memory to exa ctly simulate the exe-
cution of a universal Turing machine, U15,2. A key aspect of the ﬁnding is that it does
not require any modiﬁcation of the language model weights. I nstead, the construction
relies solely on designing a form of stored instruction comp uter that can subsequently
be programmed with a speciﬁc set of prompts.
1 Introduction
Interest in large language models has grown dramatically since the ea rly successes of GPT-
2, GPT-3 and InstructGPT [Radford et al., 2019, Brown et al., 2020, Ouyang et al., 2022],
and more recently with the popularity of ChatGPT [Schulman et al., 202 2]. Beyond simple
question answering, where an input string posing a question might elic it an output string
containing a reasonable answer, an important discovery has been t he emergence of in-context
learning, where prepending a question with a set of related (question, answ er) pairs signiﬁ-
cantly improves question answering accuracy [Radford et al., 2019]. Even adding a natural
language instruction before example pairs appears to further enh ance language model capa-
bilities [Brown et al., 2020]. More recently, chain of thought prompting has been found to
improve question answering ability in scenarios where multiple reasonin g steps are required
to arrive at a ﬁnal answer, such as answering math word problems [W ei et al., 2022b].
Despitethese results, current transformer-basedlargelangua gemodelsremainfundamen-
tally limited as they can only condition on an input string of bounded leng th, such as 4096
tokens. This makes such models formally equivalent to ﬁnite automat a, hence restricted
in the computations they can express. However, recent works ha ve begun to investigate
techniques for chaining multiple calls to a language model by processin g model outputs then
1passing thesebackassubsequent inputstothemodel. Anexampleis least to most prompting ,
whereacomplexreasoningquestionisansweredﬁrstbypromptingt hemodeltoproducesim-
pler sub-questions, then passing each sub-question and resulting answer back into the model
to help answer subsequent sub-questions, until a ﬁnal answer is r eached [Zhou et al., 2022].
Another example is work on language model cascades that investigates various strategies for
processing model outputs and feeding these as inputs to subsequ ent language model calls
[Dohan et al., 2022]. Such works raise the question of whether augme nting a language model
with an external feedback loop is merely useful, or fundamentally ex pands the range of com-
putations that can be performed. To investigate this question, we consider augmenting a
language model with an external read-write memory and ask wheth er this confers the ability
to simulate any algorithm on any input.
This paper gives an aﬃrmative answer by establishing computational universality for a
speciﬁc large language model, Flan-U-PaLM 540B [Chung et al., 2022], a ugmented with an
associativeread-writememory. Akeyaspectoftheresultisthatit isachievedbydevelopinga
simple form of stored instruction computer [von Neumann, 1945] that connects the language
model to an associative memory, then follows a simple instruction cyc le where the next
input prompt to be passed to the language model is retrieved from m emory, the output of
the language model is parsed to recover any variable assignments t hat are then stored in
the associative memory, and the next instruction is retrieved (i.e., t he next input prompt
to be provided to the language model). Each parsing step between t he language model and
memory is performed by a simple regular expression match (i.e., a ﬁnite automaton).
Once a stored instruction computer has been created, a speciﬁc “ prompt program” is
designed to drive the system to simulate a universal Turing machine U15,2[Neary, 2008,
Neary and Woods, 2009]. Proving the ﬁdelity of the simulation reduce s to checking a ﬁnite
set of prompt-result behaviours and verifying that the language m odel produces the correct
output for each of the ﬁnite set of possible input prompt strings it m ight encounter. That
is, although the overall input-output behaviour of the Flan-U-PaL M 540B model is not
fully understood, a suﬃciently reliable subset of its input-output ma p can be isolated and
controlled to simulate a universal computer. Importantly, this res ult does not involve any
additional “training” of the language model (i.e., no modiﬁcation of its pre-trained weights),
but instead relies solely on providing speciﬁc prompt strings to the mo del and parsing its
outputs to determine values to be saved in memory.
2 Stored instruction computer
As noted, there are many ways to orchestrate feedback betwee n the outputs of a language
model and subsequent input prompts [Zhou et al., 2022, Dohan et al., 2022]. In developing
a simple feedback loop we would like to minimize external processing and perform as much
of the computation with the language model as possible, while still sup porting computa-
tional universality. To achieve this, we consider a simple form of stored instruction computer
[von Neumann, 1945], where the language model plays the role of a ce ntral processing unit
(CPU), and the random access memory (RAM) is supplied by an exter nal associative mem-
2ory. Such an architecture allows for a simple interaction loop that ca n support general
computation and convenient programmability. In this architecture , the external associative
memory is a simple “dictionary”, MEMORY, that maps unique keys to values, or equivalently,
maps variable names to values, or address locations to values. Unlike the RAM in a physi-
cal computer, variable names will be strings (i.e., ﬁnite length sequen ces of symbols from a
ﬁnite alphabet) to support convenient interaction with a language m odel, while values will
be strings or integers.
Toensurethatcomputationaluniversalitydoesnotfollowsimplyfro mexternalprocessing
capability, all interaction between the language model and the memo ry will be restricted to
ﬁnite state computation , such as simple regular expression parsers.
All code below will be given in Python 3 using the standard regular expr ession library
re. Note that the regular expressions used for pre and post proces sing are rudimentary and
can easily be improved in several ways; the versions given are merely suﬃcient to establish
the main points in this paper.
2.1 Post processing language model outputs
The output string from the language model will be parsed by a simple r egular expression
that detects assignments in the form variable name = "value" , which are then applied to
the associative memory as MEMORY[variable name] = "value" . The variable assignment
functionassignments is shown below.
def assignments(string):
global MEMORY
regex = ’(?s)(?:((?:\w|\-)+)\s*=\s*(?:\"((?:.*\n)|(?: [^\"]*))\"))(.*)’
matches = re.findall(regex, string)
suffix = ’’
while len(matches) > 0:
label, value, suffix = matches[0]
MEMORY[label] = value
matches = re.findall(regex, suffix)
return suffix
Additionally, splicing is allowed in value strings before being assigned to m emory; that is,
weincludearegularexpressionparserthatdetectsoccurrences ofthepattern %[variable name]
in any value string, replacing any such occurrence with the string at memory location
variable name, i.e.,MEMORY[variable name], before assignment. The substitution func-
tionsubstitute is shown below.
3def substitute(string, char):
global MEMORY, BLANK
regex = f"(?s)(.*?)(?:{char}\[(\w+)\])(.*)"
matches = re.findall(regex, string)
string = suffix = ’’
while len(matches) > 0:
prefix, label, suffix = matches[0]
if label not in MEMORY:
MEMORY[label] = BLANK # new label has BLANK value by default
string += prefix + str(MEMORY[label])
matches = re.findall(regex, suffix)
string += suffix
return string
As a ﬁnal convenience, we also allow integer values to be stored and in cremented or
decremented. Such integer variable updating is achieved by parsing the output string for
occurrences of the pattern variable name += increment orvariable name -= decrement
then applying the updates to variable namein memory. Importantly, integer addition is
alsoaﬁnitestateoperation(see, forexample, [Sipser, 2013, Pro blem1.32]), while thePython
standard implements the bignumtype that handles arbitrarily large integers. The update
functionupdates is shown below.
def updates(string):
global MEMORY
regex = ’(\w+)\s*((?:\+|\-)=)\s*(\d+)’
matches = re.findall(regex, string)
if matches != None:
for match in matches:
label, operator, valuestring = match
sign = 1 if operator == "+=" else -1
value = int(valuestring) * sign
if label in MEMORY and isinstance(MEMORY[label], int):
MEMORY[label] += value
else:
MEMORY[label] = value # new or non-int starts from 0 by defaul t
2.2 Pre processing language model inputs
Each input prompt to the language model will be retrieved from a spe cial memory location
opand passed as a prompt to the language model in each computationa l cycle; that is,
MEMORY[’op’] will serve as an “instruction register”. Instruction branching can then be
achieved simply by assigning a diﬀerent prompt string to MEMORY[’op’] during a computa-
tional cycle.
4Toaccessstoredmemoryvalues, wealsoallowsplicingintheinputprom ptstringretrieved
fromop. In particular, the regular expression parser for the input promp t ﬁrst detects
patterns of the form @[variable name]and replaces these by splicing in the string retrieved
fromMEMORY[variable name]before passing the prompt string to the language model. For
input pre processing, it will also be convenient to allow repeated subs titutions of nested
@occurrences, so we add the repeated substitution function substitute nestedbelow.
Note that, technically, allowing arbitrarily nested substitutions can simulate a context free
grammar [Sipser, 2013], which violates the constraint of ﬁnite state computation; however,
we will only use bounded depth nesting (depth bound 2) below to ensu re the pre and post
processing steps all remain achievable by ﬁnite state computation.
def substitute_nested(string, char):
regex = f"(?s)(.*?)(?:{char}\[(\w+)\])(.*)"
while re.match(regex, string) != None:
string = substitute(string, char)
return string
2.3 Compute cycle
Finally, a stored instruction computer can run a single compute cycle : retrieve the next
prompt string from MEMORY[’op’] ; process the prompt string by possibly splicing in other
strings from memory; pass the prompt string to the language mode l; process the output
string, possibly splicing in other strings from memory, detecting all a ssignments and incre-
ment/decrement updates, applying these to memory; and repeat . Computation proceeds
until the next instruction in MEMORY[’op’] is the special instruction string ’halt’. In
particular, the main loop is constructed as follows.
def main():
global MEMORY
while True:
op = MEMORY[’op’]
if op == ’halt’:
return None
prompt = substitute_nested(op, ’@’)
result = call_llm_server(prompt)
result = substitute(result, ’%’)
suffix = assignments(result)
updates(suffix)
This main instruction loop demonstrates how the language model play s the role of the
CPU, eﬀectively taking the next instruction and its operands expre ssed by the prompt string
and acting on these to produce the result string, which is then used to update the memory.
53 Universal Turing machine
The concept of a universal computer—a computing machine that ca n simulate the execution
of any other computing machine on any input—was developed by Alan T uring to solve
theEntscheidungsproblem [Turing, 1937]. By the Church-Turing thesis, all computational
mechanisms are considered to be expressible by a Turing machine , which informally consists
of a ﬁnite state controller and an unboundedly large “tape” memory with a “head” that
can access a single tape location and move one location left or right in e ach compute cycle
[Sipser, 2013, Chapter 3].
Formally, a Turing machine consists of a tuple M= (Q,Σ,b,q0,T,f), whereQis a ﬁnite
set of states, Σ is a ﬁnite set of tape symbols, b∈Σ is the blank symbol, q0∈Qis the start
state,T⊆Q×Σis theset ofhalting(state, symbol) pairs, and f:Q×Σ→Σ×{−1,+1}×Q
is a ﬁnite set of transition rules that specify the operation of the ma chine in each compute
cycle. Weassumethetapeisbi-directionallyunbounded, somemorylo cationscanbeindexed
by an integer i∈Z. Leti0∈Zdenote the initial location of the tape head.
The execution of a Turing machine can then be deﬁned as follows. The tape memory
is initialized with a ﬁnite number of non-blank symbols with all other locat ions blank, M
starts in state q0, and the tape head starts at location i0. At the start of each compute cycle,
the tapehead isat some location i∈Z, the machine is in some state q∈Q, and some symbol
σ∈Σ is under the tape head. This combination determines the update f(q,σ)/mapsto→(σ′,m,q′),
specifying that the symbol σ′is written at the current memory location i, the machine state
qis updated to q′, and the tape head is moved one step left, to location i′=i−1 ifm=−1,
otherwise one step right, to location i′=i+1 ifm= +1. The compute cycle repeats until
the machine encounters a conﬁguration ( q,σ)∈T. Non-halting computations are possible.
[Shannon, 1956] began an eﬀort to identify the smallest universal T uring machines in
terms of the number of states and tape symbols used. A gap remain s between the known
upper and lower bounds on the state and symbol counts for a unive rsal Turing machine
[Neary, 2008, Neary and Woods, 2009], but progressively smaller un iversal Turing machines
have been identiﬁed. We will consider one such machine in this paper, U15,2, which uses only
15 states and 2 tape symbols [Neary and Woods, 2009]. This Turing ma chine is Pareto
optimal in terms of the smallest known universal Turing machines [Nea ry, 2008]. For-
mally, the Turing machine U15,2can be deﬁned by a tuple ( Q,Σ,b,q0,T,f), where Q=
{A,B,C,D,E,F,G,H,I,J,K,L,M,N,O }, Σ ={0,1},b= 0,q0=A,T={(J,1)}, and
the transition function fis deﬁned in Table 1. The initial head position i0depends on how
the memory is initialized for a given problem instance.
4 Simulating U15,2with a prompt program
In this section, we show that the stored instruction computer deﬁ ned in Section 2 can be
programmed to simulate the universal Turing machine U15,2, provided that a ﬁnite set of
conditional assignments and evaluations can be correctly perform ed by the language model.
That is, we ﬁrst propose a speciﬁc prompt program that, if execut ed correctly, exactly
6A B C D E F G H
00,+,B1,+,C0,−,G0,−,F1,+,A1,−,D0,+,H1,−,I
11,+,A1,+,A0,−,E1,−,E1,−,D1,−,D1,−,G1,−,G
I J K L M N O
00,+,A1,−,K0,+,L0,+,M0,−,B0,−,C0,+,N
11,−,Jhalt 1 ,+,N1,+,L1,+,L0,+,O1,+,N
Table 1: Transition table for the universal Turing machine U15,2. Rows are indexed by the
read symbol σ, columns are indexed by the state q, and each table entry (σ′,m,q′)speciﬁes
the write symbol σ′, the tape head move m∈ {−1,+1}, and the next state q′.
simulates U15,2. The next section will then verify that a speciﬁc large language mode l,
Flan-U-PaLM 540B, is indeed able to execute each of the program ins tructions correctly.
A prompt program consists of a ﬁnite set of pre designed strings st ored in memory that
provide input prompts to the language model as part of the main com pute cycle, via a call
tomain()outlined in Section 2. To mimic the behaviour of U15,2, we design the prompt
program as follows. First, a “boot” prompt is designed that “instru cts” the language model
about the behaviour of variable assignments, variable evaluations a fter assignment, and if-
then conditionals.
boot = """
result = " op="%[B]" %[i]="0" i+=1 "
if 0==1 then result = " op="%[A]" %[i]="1" i+=1 "
$result
" op="%[B]" %[i]="0" i+=1 "
result = " op="%[B]" %[i]="0" i+=1 "
if 1==1 then result = " op="%[A]" %[i]="1" i+=1 "
$result
" op="%[A]" %[i]="1" i+=1 "
result = " op="%[C]" %[i]="1" i+=1 "
if 0==1 then result = " op="%[A]" %[i]="1" i+=1 "
$result
" op="%[C]" %[i]="1" i+=1 "
result = " op="%[C]" %[i]="1" i+=1 "
if 1==1 then result = " op="%[A]" %[i]="1" i+=1 "
$result
" op="%[A]" %[i]="1" i+=1 "
result = " op="%[G]" %[i]="0" i-=1 "
if 0==1 then result = " op="%[E]" %[i]="0" i-=1 "
$result
" op="%[G]" %[i]="0" i-=1 "
result = " op="%[G]" %[i]="0" i-=1 "
if 1==1 then result = " op="%[E]" %[i]="0" i-=1 "
7$result
" op="%[E]" %[i]="0" i-=1 "
result = " op="%[F]" %[i]="0" i-=1 "
if 0==1 then result = " op="%[E]" %[i]="1" i-=1 "
$result
" op="%[F]" %[i]="0" i-=1 "
result = " op="%[F]" %[i]="0" i-=1 "
if 1==1 then result = " op="%[E]" %[i]="1" i-=1 "
$result
" op="%[E]" %[i]="1" i-=1 "
result = " op="%[K]" %[i]="1" i-=1 "
if 0==1 then result = " op="halt" "
$result
" op="%[K]" %[i]="1" i-=1 "
result = " op="%[K]" %[i]="1" i-=1 "
if 1==1 then result = " op="halt" "
$result
" op="halt" "
result = " op="%[L]" %[i]="0" i+=1 "
if 0==1 then result = " op="%[N]" %[i]="1" i+=1 "
$result
" op="%[L]" %[i]="0" i+=1 "
result = " op="%[L]" %[i]="0" i+=1 "
if 1==1 then result = " op="%[N]" %[i]="1" i+=1 "
$result
" op="%[N]" %[i]="1" i+=1 "
result = " op="%[C]" %[i]="0" i-=1 "
if 1==1 then result = " op="%[O]" %[i]="0" i+=1 "
$result
" op="%[O]" %[i]="0" i+=1 "
"""
Next, a series of “instruction” prompts are deﬁned. Each of thes e strings is intended to
express the logic of a corresponding Turing machine state in U15,2speciﬁed in Table 1.
A = """@[boot]result = " op="%[B]" %[i]="0" i+=1 "
if @[@[i]]==1 then result = " op="%[A]" %[i]="1" i+=1 "
$result
"""
B = """@[boot]result = " op="%[C]" %[i]="1" i+=1 "
if @[@[i]]==1 then result = " op="%[A]" %[i]="1" i+=1 "
$result
"""
8C = """@[boot]result = " op="%[G]" %[i]="0" i-=1 "
if @[@[i]]==1 then result = " op="%[E]" %[i]="0" i-=1 "
$result
"""
D = """@[boot]result = " op="%[F]" %[i]="0" i-=1 "
if @[@[i]]==1 then result = " op="%[E]" %[i]="1" i-=1 "
$result
"""
E = """@[boot]result = " op="%[A]" %[i]="1" i+=1 "
if @[@[i]]==1 then result = " op="%[D]" %[i]="1" i-=1 "
$result
"""
F = """@[boot]result = " op="%[D]" %[i]="1" i-=1 "
$result
"""
G = """@[boot]result = " op="%[H]" %[i]="0" i-=1 "
if @[@[i]]==1 then result = " op="%[G]" %[i]="1" i-=1 "
$result
"""
H = """@[boot]result = " op="%[I]" %[i]="1" i-=1 "
if @[@[i]]==1 then result = " op="%[G]" %[i]="1" i-=1 "
$result
"""
I = """@[boot]result = " op="%[A]" %[i]="0" i+=1 "
if @[@[i]]==1 then result = " op="%[J]" %[i]="1" i-=1 "
$result
"""
J = """@[boot]result = " op="%[K]" %[i]="1" i-=1 "
if @[@[i]]==1 then result = " op="halt" "
$result
"""
K = """@[boot]result = " op="%[L]" %[i]="0" i+=1 "
if @[@[i]]==1 then result = " op="%[N]" %[i]="1" i+=1 "
$result
"""
L = """@[boot]result = " op="%[M]" %[i]="0" i+=1 "
if @[@[i]]==1 then result = " op="%[L]" %[i]="1" i+=1 "
$result
"""
M = """@[boot]result = " op="%[B]" %[i]="0" i-=1 "
if @[@[i]]==1 then result = " op="%[L]" %[i]="1" i+=1 "
$result
"""
N = """@[boot]result = " op="%[C]" %[i]="0" i-=1 "
if @[@[i]]==1 then result = " op="%[O]" %[i]="0" i+=1 "
$result
"""
O = """@[boot]result = " op="%[N]" %[i]="0" i+=1 "
if @[@[i]]==1 then result = " op="%[N]" %[i]="1" i+=1 "
$result
"""
9It helps to understand how this prompt program is intended to work . First, note that the
memory location ’i’is intended to keep track of the current location of the Turing mach ine
head, so that any update i-=1will correspond to moving the head one step left, and i+=1
will correspond to moving the head one step right. Next, consider t he post processing of
one of the result strings, for example " op=%[N]" %[i]="1" i+=1 " . In this string, the
expression %[i]="1" is intended to write the target symbol ’1’to the memory location
indexed by MEMORY[’i’] . That is, during post processing, any substring of the form %[x]
will ﬁrst be replaced by the string in MEMORY[’x’] before performing any assignments, as
explained in Section 2. Thus, given %[i]="1" the substring %[i]will ﬁrst be replaced by
the value in MEMORY[’i’] , sayℓ, which then serves as the label in memory to be assigned the
value’1’. For example, if we assume MEMORY[’i’] = 42 andMEMORY[’42’] = ’0’ , then
after post processing and assignment we will have MEMORY[’42’] = ’1’ . Control branching,
i.e., a state transition, is achieved by assigning a new instruction strin g from{A, ...,O}to
the instruction register MEMORY[’op’] , as speciﬁed by the assignment string op="%[N] in
the example.
In the pre processing phase, the prompt string is obtained from MEMORY[’op’] , then
substrings of the form @[x]are replaced by the string stored in MEMORY[’x’] . This allows
for morecompact instruction strings A, ...,O, since thelengthy bootstring canjust bespliced
in during pre processing. More importantly, the symbol at the curr ent head position can
be read with @[@[i]]. To see why this works, note that the preprocessor will apply nested
substitutions of the @[x]patterns, as discussed in Section 2. Therefore, if we continue to
assume that MEMORY[’i’] = 42 andMEMORY[’42’] = ’1’ , the ﬁrst substitution of @[@[i]]
will result in @[’42’], and the second substitution will result in ’1’. That is, after pre
processing, thesubstring @[@[i]] isreplacedwiththevaluefoundin MEMORY[MEMORY[’i’]] ,
i.e., the symbol at the current position of the tape head, which in this case will be ’1’.
Given this understanding, it is easy to verify that each of the instru ction strings A, ...,
Ocorrectly mimics the logic of the corresponding states A, ...,Oin Table 1, including
conditioning on the current symbol in the head position, writing the c orrect symbol to the
current head position, moving the head position in the correct direc tion, and updating the
state by assigning the correct next instruction to ’op’.
Finally, to simulate the behaviour of U15,2, we also have to consider the initial contents
of the tape memory. Let the variable TAPEbe assigned to a string that covers the non-blank
portion of the initial memory for U15,2, which must be ﬁnitely long by deﬁnition. Also let
i=i0contain the initial position of the tape head. To simulate the Turing ma chine from
this conﬁguration, we ﬁrst initialize the associative memory, MEMORY, as follows, then simply
callmain()as speciﬁed in Section 2.
10MEMORY = {’boot’:boot}
for s in ’ABCDEFGHIJKLMNO’:
MEMORY[s] = eval(s)
for loc in range(len(TAPE)):
MEMORY[str(loc)] = TAPE[loc]
BLANK = ’0’
MEMORY[’i’] = i
MEMORY[’op’] = A
main()
We now claim that each compute cycle of main()maintains an exact equivalence with
each compute cycle of the Turing machine U15,2. The proof is by induction on the number
of compute cycles.
At initialization, there is an equivalence between: the contents of th e Turing machine
memory tape and the contents of MEMORYlabelled by location numbers (with all unassigned
locations assumed to be blank); the initial tape head location i0andi; the initial state A
and initial instruction A.
Then, inductively, assume the same equivalence holds at the onset o f a compute cycle.
The Turing machine will then beupdated according to( q,σ)/mapsto→(σ′,m,q′) following thespec-
iﬁcation in Table 1. Now assume the corresponding prompt string aft er pre processing with
the same current symbol in MEMORY[MEMORY[’i’]] (i.e., the current location of the simu-
lated tapehead) returns the correct result string after the call tocallllmserver(prompt) .
Then one can verify, on a case by case basis for each (state, symb ol) pair, that the result
string speciﬁes the same symbol to be written to the current head location, speciﬁes the
same direction to move the head, and speciﬁes the corresponding n ext instruction, thus the
equivalence is maintained at the end of the cycle.
To illustrate one of the (state, symbol) veriﬁcations, consider the ﬁrst entry in Table 1,
which speciﬁes the Turing machine update ( A,0)/mapsto→(0,+,B). Observe that the instruction
Apre processed with the same input symbol ’0’atMEMORY[MEMORY[’i’]] will return the
result string " op=%[B]" %[i]="0" i+=1 " assuming thelanguagemodel operates correctly
(veriﬁed below). In this case, the post processing phase will write t he corresponding symbol
’0’to the current memory location, move the head right +=1, and assign Bto be the next
instruction, thus maintaining the equivalence.
Similarly, if the current input is 1, the Turing machine update is ( A,1)/mapsto→(1,+,A). In
this case, observe that if the instruction Apre processed with the same input symbol ’1’at
MEMORY[MEMORY[’i’]] , the condition will be true and the result string will be " op=%[A]"
%[i]="1" i+=1 " assuming thelanguagemodel operatescorrectly. Post processin g will once
again maintain the equivalence.
A similar veriﬁcation succeeds for all 29 (state, symbol) cases. (No te that the update in
stateFdoes not depend on the input, so there is one fewer case than the t otal number of
(state, symbol) pairs.)
It remains only to verify that a language model can produce the cor rect result string
given any of the instruction strings after pre processing with the c urrent memory symbol.
115 VerifyingcorrectexecutionusingFlan-U-PaLM 540B
Wenow consider thespeciﬁc languagemodel Flan-U-PaLM540B[Chun g et al., 2022], which
isalarge540Bparametermodelthathasbeenreﬁnedwithadditiona linstructionﬁne-tuning.
To ensure deterministic computation, the decoding temperature f or the language model is
set to zero (pure greedy decoding).
To complete the argument, we now simply enumerate each of the pos sible (state, symbol)
combinations and verify that the language model produces the cor rect result string given an
input prompt string composed from the corresponding instruction and pre processed with
the corresponding input symbol. This is simply a brute force proof, c alling the language
model with each possible input prompt and verifying that the correc t result string is in-
deed returned. There are 29 cases. (So, yeah, human readable b ut not human enjoyable,
apologies.)
Veriﬁcation test 1 (state A read 0)
head = MEMORY[’i’]
MEMORY[str(head)] = ’0’
op = A
prompt = substitute_nested(op, ’@’)
result = call_llm_server(prompt)
print(op)
print(result)
Output 1
@[boot]result = " op="%[B]" %[i]="0" i+=1 "
if @[@[i]]==1 then result = " op="%[A]" %[i]="1" i+=1 "
$result
" op="%[B]" %[i]="0" i+=1 "
Veriﬁcation test 2 (state A read 1)
head = MEMORY[’i’]
MEMORY[str(head)] = ’1’
op = A
prompt = substitute_nested(op, ’@’)
result = call_llm_server(prompt)
print(op)
print(result)
Output 2
@[boot]result = " op="%[B]" %[i]="0" i+=1 "
if @[@[i]]==1 then result = " op="%[A]" %[i]="1" i+=1 "
$result
" op="%[A]" %[i]="1" i+=1 "
Veriﬁcation test 3 (state B read 0)
head = MEMORY[’i’]
MEMORY[str(head)] = ’0’
op = B
prompt = substitute_nested(op, ’@’)
result = call_llm_server(prompt)
print(op)
print(result)
12Output 3
@[boot]result = " op="%[C]" %[i]="1" i+=1 "
if @[@[i]]==1 then result = " op="%[A]" %[i]="1" i+=1 "
$result
" op="%[C]" %[i]="1" i+=1 "
Veriﬁcation test 4 (state B read 1)
head = MEMORY[’i’]
MEMORY[str(head)] = ’1’
op = B
prompt = substitute_nested(op, ’@’)
result = call_llm_server(prompt)
print(op)
print(result)
Output 4
@[boot]result = " op="%[C]" %[i]="1" i+=1 "
if @[@[i]]==1 then result = " op="%[A]" %[i]="1" i+=1 "
$result
" op="%[A]" %[i]="1" i+=1 "
Veriﬁcation test 5 (state C read 0)
head = MEMORY[’i’]
MEMORY[str(head)] = ’0’
op = C
prompt = substitute_nested(op, ’@’)
result = call_llm_server(prompt)
print(op)
print(result)
Output 5
@[boot]result = " op="%[G]" %[i]="0" i-=1 "
if @[@[i]]==1 then result = " op="%[E]" %[i]="0" i-=1 "
$result
" op="%[G]" %[i]="0" i-=1 "
Veriﬁcation test 6 (state C read 1)
head = MEMORY[’i’]
MEMORY[str(head)] = ’1’
op = C
prompt = substitute_nested(op, ’@’)
result = call_llm_server(prompt)
print(op)
print(result)
Output 6
@[boot]result = " op="%[G]" %[i]="0" i-=1 "
if @[@[i]]==1 then result = " op="%[E]" %[i]="0" i-=1 "
$result
" op="%[E]" %[i]="0" i-=1 "
Veriﬁcation test 7 (state D read 0)
13head = MEMORY[’i’]
MEMORY[str(head)] = ’0’
op = D
prompt = substitute_nested(op, ’@’)
result = call_llm_server(prompt)
print(op)
print(result)
Output 7
@[boot]result = " op="%[F]" %[i]="0" i-=1 "
if @[@[i]]==1 then result = " op="%[E]" %[i]="1" i-=1 "
$result
" op="%[F]" %[i]="0" i-=1 "
Veriﬁcation test 8 (state D read 1)
head = MEMORY[’i’]
MEMORY[str(head)] = ’1’
op = D
prompt = substitute_nested(op, ’@’)
result = call_llm_server(prompt)
print(op)
print(result)
Output 8
@[boot]result = " op="%[F]" %[i]="0" i-=1 "
if @[@[i]]==1 then result = " op="%[E]" %[i]="1" i-=1 "
$result
" op="%[E]" %[i]="1" i-=1 "
Veriﬁcation test 9 (state E read 0)
head = MEMORY[’i’]
MEMORY[str(head)] = ’0’
op = E
prompt = substitute_nested(op, ’@’)
result = call_llm_server(prompt)
print(op)
print(result)
Output 9
@[boot]result = " op="%[A]" %[i]="1" i+=1 "
if @[@[i]]==1 then result = " op="%[D]" %[i]="1" i-=1 "
$result
" op="%[A]" %[i]="1" i+=1 "
Veriﬁcation test 10 (state E read 1)
head = MEMORY[’i’]
MEMORY[str(head)] = ’1’
op = E
prompt = substitute_nested(op, ’@’)
result = call_llm_server(prompt)
print(op)
print(result)
Output 10
14@[boot]result = " op="%[A]" %[i]="1" i+=1 "
if @[@[i]]==1 then result = " op="%[D]" %[i]="1" i-=1 "
$result
" op="%[D]" %[i]="1" i-=1 "
Veriﬁcation test 11 (state F read 0 == state F read 1)
op = F
prompt = substitute_nested(op, ’@’)
result = call_llm_server(prompt)
print(op)
print(result)
Output 11
@[boot]result = " op="%[D]" %[i]="1" i-=1 "
$result
op="%[D]" %[i]="1" i-=1
Veriﬁcation test 12 (state G read 0)
head = MEMORY[’i’]
MEMORY[str(head)] = ’0’
op = G
prompt = substitute_nested(op, ’@’)
result = call_llm_server(prompt)
print(op)
print(result)
Output 12
@[boot]result = " op="%[H]" %[i]="0" i-=1 "
if @[@[i]]==1 then result = " op="%[G]" %[i]="1" i-=1 "
$result
" op="%[H]" %[i]="0" i-=1 "
Veriﬁcation test 13 (state G read 1)
head = MEMORY[’i’]
MEMORY[str(head)] = ’1’
op = G
prompt = substitute_nested(op, ’@’)
result = call_llm_server(prompt)
print(op)
print(result)
Output 13
@[boot]result = " op="%[H]" %[i]="0" i-=1 "
if @[@[i]]==1 then result = " op="%[G]" %[i]="1" i-=1 "
$result
" op="%[G]" %[i]="1" i-=1 "
Veriﬁcation test 14 (state H read 0)
head = MEMORY[’i’]
MEMORY[str(head)] = ’0’
op = H
prompt = substitute_nested(op, ’@’)
result = call_llm_server(prompt)
print(op)
print(result)
15Output 14
@[boot]result = " op="%[I]" %[i]="1" i-=1 "
if @[@[i]]==1 then result = " op="%[G]" %[i]="1" i-=1 "
$result
" op="%[I]" %[i]="1" i-=1 "
Veriﬁcation test 15 (state H read 1)
head = MEMORY[’i’]
MEMORY[str(head)] = ’1’
op = H
prompt = substitute_nested(op, ’@’)
result = call_llm_server(prompt)
print(op)
print(result)
Output 15
@[boot]result = " op="%[I]" %[i]="1" i-=1 "
if @[@[i]]==1 then result = " op="%[G]" %[i]="1" i-=1 "
$result
" op="%[G]" %[i]="1" i-=1 "
Veriﬁcation test 16 (state I read 0)
head = MEMORY[’i’]
MEMORY[str(head)] = ’0’
op = I
prompt = substitute_nested(op, ’@’)
result = call_llm_server(prompt)
print(op)
print(result)
Output 16
@[boot]result = " op="%[A]" %[i]="0" i+=1 "
if @[@[i]]==1 then result = " op="%[J]" %[i]="1" i-=1 "
$result
" op="%[A]" %[i]="0" i+=1 "
Veriﬁcation test 17 (state I read 1)
head = MEMORY[’i’]
MEMORY[str(head)] = ’1’
op = I
prompt = substitute_nested(op, ’@’)
result = call_llm_server(prompt)
print(op)
print(result)
Output 17
@[boot]result = " op="%[A]" %[i]="0" i+=1 "
if @[@[i]]==1 then result = " op="%[J]" %[i]="1" i-=1 "
$result
" op="%[J]" %[i]="1" i-=1 "
Veriﬁcation test 18 (state J read 0)
16head = MEMORY[’i’]
MEMORY[str(head)] = ’0’
op = J
prompt = substitute_nested(op, ’@’)
result = call_llm_server(prompt)
print(op)
print(result)
Output 18
@[boot]result = " op="%[K]" %[i]="1" i-=1 "
if @[@[i]]==1 then result = " op="halt" "
$result
" op="%[K]" %[i]="1" i-=1 "
Veriﬁcation test 19 (state J read 1)
head = MEMORY[’i’]
MEMORY[str(head)] = ’1’
op = J
prompt = substitute_nested(op, ’@’)
result = call_llm_server(prompt)
print(op)
print(result)
Output 19
@[boot]result = " op="%[K]" %[i]="1" i-=1 "
if @[@[i]]==1 then result = " op="halt" "
$result
" op="halt" "
Veriﬁcation test 20 (state K read 0)
head = MEMORY[’i’]
MEMORY[str(head)] = ’0’
op = K
prompt = substitute_nested(op, ’@’)
result = call_llm_server(prompt)
print(op)
print(result)
Output 20
@[boot]result = " op="%[L]" %[i]="0" i+=1 "
if @[@[i]]==1 then result = " op="%[N]" %[i]="1" i+=1 "
$result
" op="%[L]" %[i]="0" i+=1 "
Veriﬁcation test 21 (state K read 1)
head = MEMORY[’i’]
MEMORY[str(head)] = ’1’
op = K
prompt = substitute_nested(op, ’@’)
result = call_llm_server(prompt)
print(op)
print(result)
Output 21
17@[boot]result = " op="%[L]" %[i]="0" i+=1 "
if @[@[i]]==1 then result = " op="%[N]" %[i]="1" i+=1 "
$result
" op="%[N]" %[i]="1" i+=1 "
Veriﬁcation test 22 (state L read 0)
head = MEMORY[’i’]
MEMORY[str(head)] = ’0’
op = L
prompt = substitute_nested(op, ’@’)
result = call_llm_server(prompt)
print(op)
print(result)
Output 22
@[boot]result = " op="%[M]" %[i]="0" i+=1 "
if @[@[i]]==1 then result = " op="%[L]" %[i]="1" i+=1 "
$result
" op="%[M]" %[i]="0" i+=1 "
Veriﬁcation test 23 (state L read 1)
head = MEMORY[’i’]
MEMORY[str(head)] = ’1’
op = L
prompt = substitute_nested(op, ’@’)
result = call_llm_server(prompt)
print(op)
print(result)
Output 23
@[boot]result = " op="%[M]" %[i]="0" i+=1 "
if @[@[i]]==1 then result = " op="%[L]" %[i]="1" i+=1 "
$result
" op="%[L]" %[i]="1" i+=1 "
Veriﬁcation test 24 (state M read 0)
head = MEMORY[’i’]
MEMORY[str(head)] = ’0’
op = M
prompt = substitute_nested(op, ’@’)
result = call_llm_server(prompt)
print(op)
print(result)
Output 24
@[boot]result = " op="%[B]" %[i]="0" i-=1 "
if @[@[i]]==1 then result = " op="%[L]" %[i]="1" i+=1 "
$result
" op="%[B]" %[i]="0" i-=1 "
Veriﬁcation test 25 (state M read 1)
18head = MEMORY[’i’]
MEMORY[str(head)] = ’1’
op = M
prompt = substitute_nested(op, ’@’)
result = call_llm_server(prompt)
print(op)
print(result)
Output 25
@[boot]result = " op="%[B]" %[i]="0" i-=1 "
if @[@[i]]==1 then result = " op="%[L]" %[i]="1" i+=1 "
$result
" op="%[L]" %[i]="1" i+=1 "
Veriﬁcation test 26 (state N read 0)
head = MEMORY[’i’]
MEMORY[str(head)] = ’0’
op = N
prompt = substitute_nested(op, ’@’)
result = call_llm_server(prompt)
print(op)
print(result)
Output 26
@[boot]result = " op="%[C]" %[i]="0" i-=1 "
if @[@[i]]==1 then result = " op="%[O]" %[i]="0" i+=1 "
$result
" op="%[C]" %[i]="0" i-=1 "
Veriﬁcation test 27 (state N read 1)
head = MEMORY[’i’]
MEMORY[str(head)] = ’1’
op = N
prompt = substitute_nested(op, ’@’)
result = call_llm_server(prompt)
print(op)
print(result)
Output 27
@[boot]result = " op="%[C]" %[i]="0" i-=1 "
if @[@[i]]==1 then result = " op="%[O]" %[i]="0" i+=1 "
$result
" op="%[O]" %[i]="0" i+=1 "
Veriﬁcation test 28 (state O read 0)
head = MEMORY[’i’]
MEMORY[str(head)] = ’0’
op = O
prompt = substitute_nested(op, ’@’)
result = call_llm_server(prompt)
print(op)
print(result)
Output 28
19@[boot]result = " op="%[N]" %[i]="0" i+=1 "
if @[@[i]]==1 then result = " op="%[N]" %[i]="1" i+=1 "
$result
" op="%[N]" %[i]="0" i+=1 "
Veriﬁcation test 29 (state O read 1)
head = MEMORY[’i’]
MEMORY[str(head)] = ’1’
op = O
prompt = substitute_nested(op, ’@’)
result = call_llm_server(prompt)
print(op)
print(result)
Output 29
@[boot]result = " op="%[N]" %[i]="0" i+=1 "
if @[@[i]]==1 then result = " op="%[N]" %[i]="1" i+=1 "
$result
" op="%[N]" %[i]="1" i+=1 "
This completes the proof.
6 Discussion
Hopefully the reader has been convinced by this point. There are so me reﬂections on this
study that are useful to share.
Although the veriﬁcation of computational universality is straightf orward, the language
model’s behaviour was brittle. Success was not achieved with every la rge language model
considered, and eﬀort was required to engineer the prompts. For example, the instruction
stringsA, ...,Owith the substitution symbols @and%are terse and not particularly ele-
gant for humans to read, yet compactness seemed essential to g et the language model to
produce correct results. Eliciting correct evaluations of variable a ssignments was sometimes
a challenge, but by far the biggest challenge was getting the languag e model to interpret
conditionals properly. The reader might notice that the conditionals have been reduced to
if-then rather than if-then-else forms. This was not an accident: I was not able to get the
language model to reliably produce correct outputs for if-then-e lse conditionals. The diﬃ-
culty with conditionals also makes it challenging to simulate other, smalle r universal Turing
machines, such as U6,4[Neary and Woods, 2009], since this requires a series of 3 conditionals
for each state, which I could not get to work without introducing ph antom states and reduc-
ing instructions to only a single conditional, ultimately ending up with a fa r less digestible
construction. Presumably improvements in the underlying language models will mitigate
such challenges.
Earlier versions of this work considered simulating Rule 110 for a one d imensional cel-
lular automaton [Wolfram, 2002], leveraging the fact that this is know n to be a (weakly)
Turing complete [Cook, 2004]. Although far more visually appealing, Ru le 110 requires an
unbounded periodic initialization to an simulate arbitrary Turing machin e, and ultimately
20the more direct simulation of U15,2presented in this paper, which requires only a bounded
initialization, appears to be more convincing.
There is an interesting analogy to the “programming language” deve loped in Sections 2
and 4 and some of the earliest programming languages [B¨ ohm, 1954], including the ﬁrst
assembly languages [Booth and Britten, 1947]. The latter is particula rly reminiscent given
the reliance on human readable labels for branch control. It is intere sting to speculate
about what other concepts in the history of software engineering (e.g., high level languages,
modularity, libraries, etc.) might be useful for eliciting desired compu tational behaviour
from a large language model.
The result in this paper is distinct from previous studies that investig ate the computa-
tional universality of neural sequence models, such as recurrent neural networks
[Siegelmann and Sontag, 2019, Weiss et al., 2018] and Transformers [P´ erez et al., 2019],
[Bhattamishra et al., 2020, Wei et al., 2022a]. The key distinction is tha t we consider a ﬁxed
language model with frozen weights, and show how external memor y augmentation can elicit
universal computational behaviour. By contrast these past stu dies have shown how compu-
tationally universal behaviour can be recovered by manipulating the weights of the neural
network, typically using unbounded (or suﬃciently high) precision we ights to encode data
structures, like multiple stacks. An advantage of these past work s is that they do not require
anyexternal memorytodemonstrateuniversal computational b ehaviour. Ontheother hand,
these results do not apply to existing large language models without a ltering their weights
(as far as currently known). The results in this paper show that lar ge language models are
already computationally universal—as they exist currently—provide d only that they have
access to an unbounded external memory.
Acknowledgments
Sincere thanks to Noah Fiedel, Ramki Gummadi, Andr´ as Gy¨ orgy, C hris Harris, Tengyu Ma,
Jason Wei, Sherry Yang, Denny Zhou and Martin Zinkevich for essen tial discussions leading
to this work. Thanks also to Google Brain and my team members for pr oviding an ideal
environment for conducting exploratory research. Support fro m the CIFAR Canada AI
Research Chairs program, NSERC, and Amii is also gratefully acknow ledged.
References
[Bhattamishra et al., 2020] Bhattamishra, S., Patel, A., and Goyal, N. (2020). On the com-
putationalpowerofTransformersanditsimplicationsinsequence m odeling. In Conference
on Computational Natural Language Learning (CONLL) .
[B¨ ohm, 1954] B¨ ohm, C. (1954). Calculatrices digitales du d´ echiﬀrage de formules logico -
math´ ematiques par la machine mˆ eme dans la conception du pr ogramme . PhD thesis, ETH
Z¨ urich.
21[Booth and Britten, 1947] Booth, A. and Britten, K. (1947). Gene ral considerations in the
deisgn of an all purpose electronic digital computer. Technical rep ort, Institute for Ad-
vanced Study, Princeton. 2nd ed.
[Brown et al., 2020] Brown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan , J., Dhariwal,
P., Neelakantan, A., Shyam, P., Sastry, G., Askell, A., Agarwal, S., Her bert-Voss, A.,
Krueger, G., Henighan, T., Child, R., Ramesh, A., Ziegler, D., Wu, J., Wint er, C., Hesse,
C., Chen, M., Sigler, E., Litwin, M., Gray, S., Chess, B., Clark, J., Berner , C., McCandlish,
S., Radford, A., Sutskever, I., and Amodei, D. (2020). Language m odels are few-shot
learners. In Advances in Neural Information Processing Systems (NeurIP S).
[Chung et al., 2022] Chung, H. W., Hou, L., Longpre, S., Zoph, B., Tay, Y., Fedus, W., Li,
Y., Wang, X., Dehghani, M., Brahma, S., Webson, A., Gu, S. S., Dai, Z., Su zgun, M.,
Chen, X., Chowdhery, A., Castro-Ros, A., Pellat, M., Robinson, K., Va lter, D., Narang,
S., Mishra, G., Yu, A., Zhao, V., Huang, Y., Dai, A., Yu, H., Petrov, S., Ch i, E., Dean, J.,
Devlin, J., Roberts, A., Zhou, D., Le, Q., and Wei, J. (2022). Scaling ins truction-ﬁnetuned
language models. arXiv 2210.11416.
[Cook, 2004] Cook, M. (2004). Universality in elementary cellular aut omata.Complex Sys-
tems, 15:1–40.
[Dohan et al., 2022] Dohan, D., Xu, W., Lewkowycz, A., Austin, J., Biebe r, D., Lopes,
R. G., Wu, Y., Michalewski, H., Saurous, R. A., Sohl-dickstein, J., Murph y, K., and
Sutton, C. (2022). Language model cascades. arXiv 2207.10342 .
[Neary, 2008] Neary, T. (2008). Small universal Turing machines . PhD thesis, National
University of Ireland, Maynooth.
[Neary and Woods, 2009] Neary, T. and Woods, D. (2009). Four sm all universal Turing
machines. Fundamenta Informaticae , 91:105–126.
[Ouyang et al., 2022] Ouyang, L., Wu, J., Jiang, X., Almeida, D., Wainwrigh t, C., Mishkin,
P., Zhang, C., Agarwal, S., Slama, K., Gray, A., Schulman, J., Hilton, J., K elton, F.,
Miller, L., Simens, M., Askell, A., Welinder, P., Christiano, P., Leike, J., and Lowe, R.
(2022). Training languagemodels tofollowinstructions withhumanfe edback. In Advances
in Neural Information Processing Systems (NeurIPS) .
[P´ erez et al., 2019] P´ erez, J., Marinkovi´ c, J., and Parcel´ o, P. ( 2019). On the Turing com-
pleteness ofmodernneuralnetworkarchitectures. In International Conference on Learning
Representations (ICLR) .
[Radford et al., 2019] Radford, A., Wu, J., Child, R., Luan, D., Amodei, D ., and Sutskever,
I. (2019). Language models are unsupervised multitask learners.
[Schulman et al., 2022] Schulman, J., Zoph, B., Kim, C., Hilton, J., Menick, J., Weng, J.,
Uribe, J. F. C., Fedus, L., Metz, L., Pokorny, M., Lopes, R. G., Zhao, S., Vijayvergiya, A.,
22Sigler, E., Perelman, A., Voss, C., Heaton, M., Parish, J., Cummings, D., Nayak, R., Bal-
com, V., Schnurr, D., Kaftan, T., Hallacy, C., Turley, N., Deutsch, N., andGoel, V.(2022).
ChatGPT: Optimizing language models for dialogue. https://openai.co m/blog/chatgpt.
[Shannon, 1956] Shannon, C. (1956). A universal Turing machine w ith two internal states.
Automata Studies, Annals of Mathematics Studies , 34:157–165.
[Siegelmann and Sontag, 2019] Siegelmann, H.andSontag, E.(2019 ). Onthecomputational
power of neural nets. In Conference on Learning Theory (COLT) .
[Sipser, 2013] Sipser, M. (2013). Introduction to the Theory of Computation . CengageLearn-
ing, 3rd edition.
[Turing, 1937] Turing, A. (1937). On computable numbers, with an a pplication to the
Entscheidungsproblem. Proceedings of the London Mathematical Society, 2 , 42:230–265.
[von Neumann, 1945] von Neumann, J. (1945). First draft of a rep ort on the EDVAC.
[Wei et al., 2022a] Wei, C., Chen, Y., and Ma, T. (2022a). Statistically m eaningful approx-
imation: a case study on approximating Turing machines with Transfo rmers. In Advances
in Neural Information Processing Systems (NeurIPS) .
[Wei et al., 2022b] Wei, J., Wang, X., Schuurmans, D., Bosma, M., Ichte r, B., Xia, F., Chi,
E., Le, Q., and Zhou, D. (2022b). Chain of thought prompting elicits r easoning in large
language models. In Advances in Neural Information Processing Systems (NeurIP S).
[Weiss et al., 2018] Weiss, G., Goldberg, Y., and Yahav, E. (2018). On t he practical com-
putational power of ﬁnite precision RNNs for language recognition. InConference of the
Association for Computational Linguistics (ACL) .
[Wolfram, 2002] Wolfram, S. (2002). A New Kind of Science . Wolfram Media.
[Zhou et al., 2022] Zhou, D., Sch¨ arli, N., How, L., Wei, J., Scales, N., Wan g, X., Schuur-
mans, D., Cui, C., Bousquet, O., Le, Q., and Chi, E. (2022). Least-to -most prompting
enables complex reasoning in large language models. arXiv 2205.10625 .
23